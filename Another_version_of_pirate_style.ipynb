{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHx5uWUvTbawVC94mtq5rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabarishraja/Transformer-pirate-style-generator/blob/main/Another_version_of_pirate_style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pmgUeghLXuX0",
        "outputId": "ff4d2287-8753-49c2-aabf-1fe1f7588129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m356.9/356.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.42.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.9.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.8.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en_core_web_sm 3.8.0\n",
            "    Uninstalling en_core_web_sm-3.8.0:\n",
            "      Successfully uninstalled en_core_web_sm-3.8.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U spacy==3.7.4 textacy==0.13.0 spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, csv, textwrap, requests\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_infix_regex\n",
        "from spacy.symbols import ORTH\n",
        "import textacy\n",
        "import textacy.preprocessing as tprep\n",
        "from textacy.extract import ngrams\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "PNc1-iU7X-pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOOKS = [\n",
        "    (\"Treasure Island (Stevenson)\", \"https://www.gutenberg.org/cache/epub/120/pg120.txt\"),\n",
        "    (\"Captain Blood (Sabatini)\", \"https://www.gutenberg.org/ebooks/1965.txt.utf-8\"),\n",
        "    (\"Captain Singleton (Defoe)\", \"https://www.gutenberg.org/ebooks/6422.txt.utf-8\"),\n",
        "    (\"General History of the Pyrates (Johnson/Defoe)\", \"https://www.gutenberg.org/ebooks/40580.txt.utf-8\"),\n",
        "    (\"Howard Pyle's Book of Pirates\", \"https://www.gutenberg.org/ebooks/26862.txt.utf-8\"),\n",
        "]\n",
        "print(\"Books loaded:\", len(BOOKS))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8HUCebaYA6-",
        "outputId": "57ba9485-7929-49a3-cc77-876dbd70b253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books loaded: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strip to the start content of book"
      ],
      "metadata": {
        "id": "SUQUG8oSY0Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_text(url):\n",
        "    r = requests.get(url, timeout=60); r.raise_for_status(); return r.text\n",
        "\n",
        "def strip_gutenberg(txt):\n",
        "    s = re.search(r\"\\*\\*\\* START OF(.*)\\*\\*\\*\", txt)\n",
        "    e = re.search(r\"\\*\\*\\* END OF(.*)\\*\\*\\*\", txt)\n",
        "    if s and e and s.end() < e.start():\n",
        "        txt = txt[s.end():e.start()]\n",
        "    return txt.replace(\"\\r\\n\",\"\\n\").strip()\n",
        "\n",
        "def light_preprocess(s):\n",
        "    s = tprep.normalize.unicode(s, form=\"NFKC\")\n",
        "    s = tprep.normalize.quotation_marks(s)\n",
        "    s = tprep.normalize.hyphenated_words(s)\n",
        "    s = tprep.normalize.whitespace(s)\n",
        "    return s.strip()\n",
        "\n",
        "# TEST\n",
        "raw = fetch_text(BOOKS[3][1])\n",
        "core = strip_gutenberg(raw)\n",
        "clean = light_preprocess(core)\n",
        "print(\"Sample clean excerpt:\\n\", clean[:300])\n",
        "print(\"Chars:\", len(clean))\n"
      ],
      "metadata": {
        "id": "aGklzqENYDru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy pipeline with \"piratey\" tokenizer"
      ],
      "metadata": {
        "id": "kTTQ1Vj6Y6KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nlp():\n",
        "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
        "    if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
        "        nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "    APOST = {\"'\", \"â€™\"}\n",
        "    infixes = [i for i in nlp.Defaults.infixes if not any(ch in i for ch in APOST)]\n",
        "    infix_re = compile_infix_regex(infixes)\n",
        "\n",
        "    def tok(nlp_):\n",
        "        return Tokenizer(\n",
        "            nlp_.vocab,\n",
        "            rules=nlp_.Defaults.tokenizer_exceptions,\n",
        "            prefix_search=nlp_.tokenizer.prefix_search,\n",
        "            suffix_search=nlp_.tokenizer.suffix_search,\n",
        "            infix_finditer=infix_re.finditer,\n",
        "            token_match=nlp_.tokenizer.token_match,\n",
        "            url_match=nlp_.tokenizer.url_match,\n",
        "        )\n",
        "    nlp.tokenizer = tok(nlp)\n",
        "    for sc in [\"Cap'n\",\"Capâ€™n\",\"'em\",\"â€™em\",\"d'ye\",\"dâ€™ye\",\"o'\",\"oâ€™\",\"aye-aye\"]:\n",
        "        nlp.tokenizer.add_special_case(sc, [{ORTH: sc}])\n",
        "    return nlp\n",
        "\n",
        "# TEST\n",
        "nlp = build_nlp()\n",
        "test = \"Capâ€™n Flintâ€™s crewâ€”dâ€™ye hear â€™em? oâ€™ rum, aye-aye! Ahoy, matey!\"\n",
        "print([t.text for t in nlp.tokenizer(test)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_16IjuDYkHU",
        "outputId": "8a7f67c4-d8c1-4774-9b6d-a61ff72e0fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Capâ€™n', 'Flint', 'â€™s', 'crew', 'â€”', 'dâ€™ye', 'hear', 'â€™em', '?', 'oâ€™', 'rum', ',', 'aye-aye', '!', 'Ahoy', ',', 'matey', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token View"
      ],
      "metadata": {
        "id": "AnDh6rv0ZGsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import STOP_WORDS as EN_STOP\n",
        "\n",
        "pirate_keep = {\"aye\",\"yo-ho-ho\",\"matey\",\"capâ€™n\",\"cap'n\",\"rum\",\"ahoy\",\"avast\",\"aye-aye\",\"blackbeard\",\"flint\",\"silver\"}\n",
        "custom_stop = set(EN_STOP) - pirate_keep\n",
        "\n",
        "def token_view(doc, lowercase=True, remove_stop=True, remove_punct=True, remove_num=True, use_lemma=True):\n",
        "    out = []\n",
        "    for t in doc:\n",
        "        if t.is_space: continue\n",
        "        if remove_punct and t.is_punct: continue\n",
        "        if remove_num and t.like_num: continue\n",
        "        s = t.lemma_ if use_lemma else t.text\n",
        "        s = s.lower() if lowercase else s\n",
        "        if remove_stop and s in custom_stop: continue\n",
        "        s = s.strip()\n",
        "        if s: out.append(s)\n",
        "    return out\n",
        "\n",
        "def stats_from_doc(doc):\n",
        "    toks = token_view(doc)\n",
        "    vocab = Counter(toks)\n",
        "    sents = [s for s in doc.sents if s.text.strip()]\n",
        "    avg_len = np.mean([sum(1 for t in s if not (t.is_space or t.is_punct)) for s in sents]) if sents else 0.0\n",
        "    return {\n",
        "        \"sentences\": len(sents),\n",
        "        \"tokens\": len(toks),\n",
        "        \"vocab\": len(vocab),\n",
        "        \"avg_sent_len_tokens\": float(avg_len),\n",
        "        \"top_tokens_15\": vocab.most_common(15),\n",
        "    }\n",
        "\n",
        "# TEST\n",
        "doc = nlp(clean)\n",
        "print(stats_from_doc(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE1yXg6kZCwM",
        "outputId": "44e546dc-df04-495d-b903-52a8f42c81e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentences': 5172, 'tokens': 52035, 'vocab': 7681, 'avg_sent_len_tokens': 25.22505800464037, 'top_tokens_15': [('ship', 578), ('captain', 490), ('pyrates', 423), ('come', 386), ('man', 367), ('men', 365), ('time', 325), ('sloop', 310), ('great', 276), ('island', 256), ('find', 250), ('board', 248), ('day', 240), ('good', 233), ('pyrate', 208)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Piratey Term finder"
      ],
      "metadata": {
        "id": "_xYaTdRMZlvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pirate_patterns = [\n",
        "    r\"\\baye\\b\", r\"\\bmatey\\b\", r\"\\byo-?ho-?ho\\b\", r\"cap[â€™']n\",\n",
        "    r\"d[â€™']ye\", r\"[â€™']em\", r\"\\bahoy\\b\", r\"\\bavast\\b\", r\"\\brum\\b\",\n",
        "]\n",
        "\n",
        "def pirate_counts_and_kwic(text, patterns=pirate_patterns, width=35, max_hits=3):\n",
        "    counts = {pat: len(re.findall(pat, text, flags=re.IGNORECASE)) for pat in patterns}\n",
        "    snips = {}\n",
        "    for pat in patterns:\n",
        "        lines = []\n",
        "        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
        "            start = max(0, m.start()-width)\n",
        "            end   = min(len(text), m.end()+width)\n",
        "            lines.append(text[start:end].replace(\"\\n\",\" \"))\n",
        "            if len(lines) >= max_hits: break\n",
        "        if lines:\n",
        "            snips[pat] = lines\n",
        "    return counts, snips\n",
        "\n",
        "# TEST\n",
        "counts, snips = pirate_counts_and_kwic(clean)\n",
        "print(\"Pirate counts (nonzero):\", {k:v for k,v in counts.items() if v>0})\n",
        "for pat, lines in snips.items():\n",
        "    print(f\"\\nKWIC {pat}:\")\n",
        "    for ln in lines: print(\"  â€¦\" + ln + \"â€¦\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEloKm-KZMOb",
        "outputId": "7fe1be8c-abaa-49c2-cd6d-ec91c73ce6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pirate counts (nonzero): {\"d[â€™']ye\": 1, \"[â€™']em\": 7, '\\\\brum\\\\b': 17}\n",
            "\n",
            "KWIC d[â€™']ye:\n",
            "  â€¦ip will hear some Reason. _Judge._ D'ye hear how the Scoundrel prates?--Whâ€¦\n",
            "\n",
            "KWIC [â€™']em:\n",
            "  â€¦ile, till I bring other Company to 'em. In the Month of _November_, 1716,â€¦\n",
            "  â€¦ for we never heard what became of 'em afterwards: Captain _Hume_ releaseâ€¦\n",
            "  â€¦ding their Superiority, we engaged 'em both about three Hours, during whiâ€¦\n",
            "\n",
            "KWIC \\brum\\b:\n",
            "  â€¦ and about four hundred Gallons of Rum, besides his Provisions and Storesâ€¦\n",
            "  â€¦ enough, and made them Presents of Rum and Sugar, in Recompence of what hâ€¦\n",
            "  â€¦ with his own Hand.-- _Such a Day, Rum all out:--Our Company somewhat sobâ€¦\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_book(title, url, nlp):\n",
        "    raw   = fetch_text(url)\n",
        "    core  = strip_gutenberg(raw)\n",
        "    clean = light_preprocess(core)\n",
        "    doc   = nlp(clean)\n",
        "    stats = stats_from_doc(doc)\n",
        "\n",
        "    # top n-grams\n",
        "    bi = Counter([sp.text.lower() for sp in ngrams(doc, n=2, filter_stops=True, filter_punct=True, filter_nums=True)])\n",
        "    tri = Counter([sp.text.lower() for sp in ngrams(doc, n=3, filter_stops=True, filter_punct=True, filter_nums=True)])\n",
        "\n",
        "    # piratey terms\n",
        "    counts, snips = pirate_counts_and_kwic(clean)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"chars\": len(clean),\n",
        "        \"stats\": stats,\n",
        "        \"bigrams_top10\": bi.most_common(10),\n",
        "        \"trigrams_top10\": tri.most_common(10),\n",
        "        \"pirate_counts\": counts,\n",
        "        \"pirate_kwic\": snips,\n",
        "    }\n",
        "\n",
        "# TEST on Treasure Island only\n",
        "one = summarize_book(*BOOKS[0], nlp)\n",
        "print(one[\"title\"], \"chars:\", one[\"chars\"])\n",
        "print(\"stats:\", one[\"stats\"])\n",
        "print(\"bigrams:\", one[\"bigrams_top10\"][:5])\n",
        "print(\"pirate (nonzero):\", {k:v for k,v in one[\"pirate_counts\"].items() if v>0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flCrr_ewZuXa",
        "outputId": "5413cae8-ee61-4b39-e666-9ebd4c050fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treasure Island (Stevenson) chars: 361727\n",
            "stats: {'sentences': 3660, 'tokens': 25841, 'vocab': 4382, 'avg_sent_len_tokens': 19.223224043715845, 'top_tokens_15': [('man', 367), ('captain', 236), ('silver', 223), ('like', 214), ('come', 211), ('hand', 197), ('doctor', 175), ('good', 151), ('know', 140), ('time', 139), ('cry', 137), ('look', 135), ('ship', 134), ('long', 127), ('think', 127)]}\n",
            "bigrams: [('dr. livesey', 44), ('ben gunn', 38), ('long john', 37), ('captain smollett', 29), ('said silver', 27)]\n",
            "pirate (nonzero): {'\\\\baye\\\\b': 27, '\\\\bmatey\\\\b': 4, '\\\\byo-?ho-?ho\\\\b': 8, \"cap[â€™']n\": 48, \"[â€™']em\": 31, '\\\\bahoy\\\\b': 1, '\\\\bavast\\\\b': 2, '\\\\brum\\\\b': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(clean)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "\n",
        "def encode(s: str):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(ixs):\n",
        "    return \"\".join(itos[i] for i in ixs)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Sample mapping:\", list(stoi.items())[:15])\n",
        "print(\"Endcoding a text(Cap'n): \", encode(\"Cap'n\"))\n",
        "print(\"Encode/Decode test:\", decode(encode(\"Cap'n\")) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO2QDcVuZ3et",
        "outputId": "58f972c7-ae82-4bb3-c6cc-ba69200b517f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 88\n",
            "Sample mapping: [('\\n', 0), (' ', 1), ('!', 2), ('&', 3), (\"'\", 4), ('(', 5), (')', 6), ('*', 7), ('+', 8), (',', 9), ('-', 10), ('.', 11), ('0', 12), ('1', 13), ('2', 14)]\n",
            "Endcoding a text(Cap'n):  [27, 54, 69, 4, 67]\n",
            "Encode/Decode test: Cap'n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting Bilanguage model"
      ],
      "metadata": {
        "id": "MgiEZEmW9IOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math, random, time"
      ],
      "metadata": {
        "id": "J21FwAdk9qNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building a single corpus for all 5 books\n",
        "def build_corpus_from_books(books):\n",
        "    pieces = []\n",
        "    for title, url in books:\n",
        "        raw   = fetch_text(url)\n",
        "        core  = strip_gutenberg(raw)\n",
        "        clean = light_preprocess(core)   # <-- your cleaner; keeps punctuation\n",
        "        pieces.append(clean)\n",
        "    return \"\\n\\n\".join(pieces)\n",
        "\n",
        "corpus = build_corpus_from_books(BOOKS)\n",
        "print(\"Total chars in corpus:\", len(corpus))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQmPP97I9VCl",
        "outputId": "890ddfef-5267-4270-8a28-a8b47e686390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chars in corpus: 2806865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rebuilding character vocab\n",
        "chars = sorted(list(set(corpus)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "\n",
        "def encode(s: str):\n",
        "  return torch.tensor([stoi[c] for c in s], dtype = torch.long)\n",
        "\n",
        "def decode(ixs):\n",
        "    return \"\".join(itos[int(i)] for i in ixs)\n",
        "\n",
        "data = encode(corpus)\n",
        "print(\"Vocab size:\", vocab_size, \"| data shape:\", tuple(data.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsgP_RyH-r7f",
        "outputId": "a8b8c967-96f7-406c-daaf-eb77261fe99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 93 | data shape: (2806865,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "vkqnjX-rDjzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "split = int(0.9 * len(data))\n",
        "train_data = data[:split]\n",
        "val_data   = data[split:]"
      ],
      "metadata": {
        "id": "yXj_ih17DeTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(split_name):\n",
        "    src = train_data if split_name == \"train\" else val_data\n",
        "    ix = torch.randint(0, len(src) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([src[i:i+block_size]     for i in ix])\n",
        "    y = torch.stack([src[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "bJQv0csSMm3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)  # (B, T, V)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, V = logits.shape\n",
        "            logits = logits.view(B*T, V)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, start_idx, max_new_tokens):\n",
        "        idx = start_idx\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)         # (B, T, V)\n",
        "            logits = logits[:, -1, :]     # (B, V)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_tok = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_tok], dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "F1ATP-AzNKm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "1IfeXgD4NOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, betas=(0.9, 0.95), weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "HptCbYVRNTEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(eval_iters=200):\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    for split_name in [\"train\", \"val\"]:\n",
        "        vals = []\n",
        "        for _ in range(eval_iters):\n",
        "            X, Y = get_batch(split_name)\n",
        "            _, loss = model(X, Y)\n",
        "            vals.append(loss.item())\n",
        "        losses[split_name] = sum(vals) / len(vals)\n",
        "    model.train()\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "UZ3widSdNU8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "grad_clip = 1.0\n",
        "\n",
        "t0 = time.time()\n",
        "for step in range(max_iters + 1):\n",
        "    if step % eval_interval == 0:\n",
        "        losses = estimate_loss(eval_iters=200)\n",
        "        print(f\"[{step:5d}] train {losses['train']:.4f} | val {losses['val']:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    X, Y = get_batch(\"train\")\n",
        "    _, loss = model(X, Y)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T-AtFCrNZtc",
        "outputId": "736f95e1-8ca0-465b-9bae-600ce08859d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0] train 5.0195 | val 5.0253 | 2.1s\n",
            "[  500] train 3.2599 | val 3.2606 | 10.4s\n",
            "[ 1000] train 2.5863 | val 2.5896 | 21.1s\n",
            "[ 1500] train 2.4747 | val 2.4773 | 29.8s\n",
            "[ 2000] train 2.4608 | val 2.4639 | 37.4s\n",
            "[ 2500] train 2.4610 | val 2.4646 | 44.3s\n",
            "[ 3000] train 2.4561 | val 2.4621 | 51.8s\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    start = torch.randint(vocab_size, (1, 1))\n",
        "    out = model.generate(start, max_new_tokens=800)[0].tolist()\n",
        "    sample = decode(out)\n",
        "\n",
        "print(\"\\n=== SAMPLE OUTPUT ===\\n\")\n",
        "print(sample[:1200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr_gpSqFNcdi",
        "outputId": "206a7a00-329d-480e-a123-d13a20b2dff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SAMPLE OUTPUT ===\n",
            "\n",
            "Ìusilee. op theed mecharde, asied. gicof ss aitan od?\"y liswie t ofambed ondeas_Cou[I he he ordld atowethed tho me senoind in, sowin theys herof asuze rulil.\n",
            "_.\"Anen t merend oure wamen befo\n",
            "oum cre 2, Yo' bullad owowarmat angrthes, e hime a fot Band, hare Jandstosur g pind st Cond I se t, paner Apprs. the Shensod the Credmerend, Pe\n",
            "Dout he arm CÃ¦thm werschaindanonde fird llanvend fr.\n",
            "\"\n",
            "I otomort pe t An, f ajundasom y, whed cads Ang Inged purs thefot rned nhe hels\n",
            "Afon he Enghe ch l cthe tsinth\n",
            "Fl ney t t s cher\n",
            "manowhice bofit_Dunginomshilund weand--I cthasth ounon he\n",
            "Chivecquck Feva o t w\n",
            "waitsse he wed urad; altaspinrunuerin wand, d his t s onghige henove athe.\"Me t r f\n",
            "n here wor ie he-pteldrdrse no aifr alll. Tustr Cat acy d tes allinsst; foulan s whicoo ger s ak h s, as,\"\n",
            "t oas! ng f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Transformer model"
      ],
      "metadata": {
        "id": "UXp8kdSHQr55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, context_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # causal mask: lower-triangular (no looking ahead)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size, dtype=torch.bool)))\n",
        "        self.scale = head_size ** -0.5    # scaled dot-product (more stable)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)                      # (B,T,hd)\n",
        "        q = self.query(x)                    # (B,T,hd)\n",
        "        # attention scores\n",
        "        wei = (q @ k.transpose(-2, -1)) * self.scale   # (B,T,T)\n",
        "        wei = wei.masked_fill(~self.tril[:T, :T], float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.drop(wei)                 # dropout on attention weights\n",
        "\n",
        "        v = self.value(x)                    # (B,T,hd)\n",
        "        out = wei @ v                        # (B,T,hd)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "f9TTLbpM0TUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embd, context_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, context_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd, bias=False) # mix heads\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # concat along channel dim, then project back\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B,T,n_embd)\n",
        "        out = self.drop(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "Y3j5sLdW0ZuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" linear -> ReLU -> linear (small/fast) \"\"\"\n",
        "    def __init__(self, n_embd, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "uUSQgGd00bPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, context_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.sa  = MultiHeadAttention(n_head, head_size, n_embd, context_size, dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ffw = FeedFoward(n_embd, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))  # residual 1\n",
        "        x = x + self.ffw(self.ln2(x)) # residual 2\n",
        "        return x"
      ],
      "metadata": {
        "id": "rcpan6e_0eIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, n_embd=192, context_size=64, n_head=3, n_layer=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n",
        "        self.context_size = context_size\n",
        "\n",
        "        # token + position embeddings\n",
        "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
        "\n",
        "        # stack of transformer blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embd, n_head=n_head, context_size=context_size, dropout=dropout)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "\n",
        "        self.ln_f   = nn.LayerNorm(n_embd)     # final norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # positions 0..T-1\n",
        "        pos = torch.arange(T, device=idx.device)\n",
        "        # embed tokens + positions\n",
        "        x = self.token_embedding_table(idx) + self.position_embedding_table(pos)  # (B,T,C)\n",
        "        # transformer stack\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        # logits over vocab\n",
        "        logits = self.lm_head(x)  # (B,T,V)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "        # autoregressive sampling (keep last context_size)\n",
        "        for _ in range(max_new_tokens):\n",
        "            x = idx[:, -self.context_size:]\n",
        "            logits, _ = self(x)\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "sdMCcEzj0hBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def quick_eval(model, iters=10):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for _ in range(iters):\n",
        "        X, Y = get_batch('val')\n",
        "        _, loss = model(X, Y)\n",
        "        losses.append(loss.item())\n",
        "    model.train()\n",
        "    return sum(losses)/len(losses)"
      ],
      "metadata": {
        "id": "k5Q339Fl0ld7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fast(model,\n",
        "               lr=5e-4,\n",
        "               max_iters=1200,\n",
        "               eval_every=300,\n",
        "               mini_ping=100,\n",
        "               sample_tokens=500):\n",
        "    print(\"ğŸš€ training (tiny transformer)\")\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9,0.95), weight_decay=1e-2)\n",
        "\n",
        "    t0 = time.time()\n",
        "    for step in range(max_iters + 1):\n",
        "        if step % mini_ping == 0 and step % eval_every != 0:\n",
        "            print(f\"  ...step {step}\")\n",
        "\n",
        "        if step % eval_every == 0:\n",
        "            val = quick_eval(model, iters=10)\n",
        "            print(f\"[{step:4d}] val {val:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "        X, Y = get_batch('train')\n",
        "        _, loss = model(X, Y)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    print(\"âœ… done. samplingâ€¦\")\n",
        "    with torch.no_grad():\n",
        "        start = torch.randint(vocab_size, (1,1))\n",
        "        out = model.generate(start, max_new_tokens=sample_tokens)[0].tolist()\n",
        "    print(\"\\n=== TRANSFORMER SAMPLE OUTPUT ===\\n\")\n",
        "    print(decode(out))"
      ],
      "metadata": {
        "id": "AvODNQ_e0nFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 64\n",
        "block_size = context_size\n",
        "m = BigramLanguageModel(vocab_size, n_embd=192, context_size=context_size, n_head=3, n_layer=3, dropout=0.1)\n",
        "train_fast(m, max_iters=1200, eval_every=300, mini_ping=100, sample_tokens=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14jhChwk0pe2",
        "outputId": "635be750-974b-4301-efd9-599be03e49db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ training (tiny transformer)\n",
            "[   0] val 4.642 | 1.4s\n",
            "  ...step 100\n",
            "  ...step 200\n",
            "[ 300] val 2.253 | 133.4s\n",
            "  ...step 400\n",
            "  ...step 500\n",
            "[ 600] val 2.028 | 264.6s\n",
            "  ...step 700\n",
            "  ...step 800\n",
            "[ 900] val 1.936 | 393.2s\n",
            "  ...step 1000\n",
            "  ...step 1100\n",
            "[1200] val 1.870 | 523.5s\n",
            "âœ… done. samplingâ€¦\n",
            "\n",
            "=== TRANSFORMER SAMPLE OUTPUT ===\n",
            "\n",
            "y the three to go them of the sucalfound, it mark geney been-ous to\n",
            "thin ander let the men to cond there, hiss dind, and were conquast\n",
            "andsted jectiction, all bult the prond_'d mantegen _Pyle_; Gunl_ Lived dirkt, teen Mark ship the\n",
            "ment and, and lif the To Mitusted bood. Wors, is abledely. WIll seme\n",
            "Sporlood one, with, the then leations prouse of a good. Mass, hart's clobely, threase agoon bleds\n",
            "erenges, Cat Willinly-you evelys thing doner throught, notshed; 172\n",
            "\"Prysiails a ling a Peor. Ol the shiccoof therd, that of the Ben of the\n",
            "siefar and thech, I share therelverain.\n",
            "Wor the Gungr. It aido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBbp84jz0rFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}