{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP06EWofhWkzRfA0TdA8REk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabarishraja/Transformer-pirate-style-generator/blob/main/Transformer_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pmgUeghLXuX0",
        "outputId": "7da8bc01-4773-4646-8314-1b081fa6cc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.9/355.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.42.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.9.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.12/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.8.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.12/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en_core_web_sm 3.8.0\n",
            "    Uninstalling en_core_web_sm-3.8.0:\n",
            "      Successfully uninstalled en_core_web_sm-3.8.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U spacy==3.7.4 textacy==0.13.0 spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, csv, textwrap, requests\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_infix_regex\n",
        "from spacy.symbols import ORTH\n",
        "import textacy\n",
        "import textacy.preprocessing as tprep\n",
        "from textacy.extract import ngrams\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "PNc1-iU7X-pi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOOKS = [\n",
        "    (\"Treasure Island (Stevenson)\", \"https://www.gutenberg.org/cache/epub/120/pg120.txt\"),\n",
        "    (\"Captain Blood (Sabatini)\", \"https://www.gutenberg.org/ebooks/1965.txt.utf-8\"),\n",
        "    (\"Captain Singleton (Defoe)\", \"https://www.gutenberg.org/ebooks/6422.txt.utf-8\")\n",
        "]\n",
        "print(\"Books loaded:\", len(BOOKS))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8HUCebaYA6-",
        "outputId": "d8b9a2f8-4d40-4cde-e2a2-1b50d3aa2ed1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books loaded: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strip to the start content of book"
      ],
      "metadata": {
        "id": "SUQUG8oSY0Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_text(url):\n",
        "    r = requests.get(url, timeout=60); r.raise_for_status(); return r.text\n",
        "\n",
        "def strip_gutenberg(txt):\n",
        "    s = re.search(r\"\\*\\*\\* START OF(.*)\\*\\*\\*\", txt)\n",
        "    e = re.search(r\"\\*\\*\\* END OF(.*)\\*\\*\\*\", txt)\n",
        "    if s and e and s.end() < e.start():\n",
        "        txt = txt[s.end():e.start()]\n",
        "    return txt.replace(\"\\r\\n\",\"\\n\").strip()\n",
        "\n",
        "def light_preprocess(s):\n",
        "    s = tprep.normalize.unicode(s, form=\"NFKC\")\n",
        "    s = tprep.normalize.quotation_marks(s)\n",
        "    s = tprep.normalize.hyphenated_words(s)\n",
        "    s = tprep.normalize.whitespace(s)\n",
        "    return s.strip()\n",
        "\n",
        "# TEST\n",
        "raw = fetch_text(BOOKS[2][1])\n",
        "core = strip_gutenberg(raw)\n",
        "clean = light_preprocess(core)\n",
        "print(\"Sample clean excerpt:\\n\", clean[:300])\n",
        "print(\"Chars:\", len(clean))\n"
      ],
      "metadata": {
        "id": "aGklzqENYDru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf17630-845b-4ea0-80b9-21ec826b2f87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample clean excerpt:\n",
            " Produced by Tom Allen, Charles Franks and the Online\n",
            "Distributed Proofreading Team\n",
            "CAPTAIN SINGLETON\n",
            "By Daniel Defoe\n",
            "With An Introduction By Edward Garnett\n",
            "[Transcriber's Note: In the print copy, the following words and those of\n",
            "the title page are written in intricate, illuminated calligraphy.]\n",
            "A TA\n",
            "Chars: 592374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build combined corpus (mentor expects a single input.txt)\n",
        "clean_texts = []\n",
        "for title, url in BOOKS:\n",
        "    raw   = fetch_text(url)\n",
        "    core  = strip_gutenberg(raw)\n",
        "    clean = light_preprocess(core)\n",
        "    clean_texts.append(clean)\n",
        "\n",
        "combined = \"\\n\\n\".join(clean_texts)\n",
        "\n",
        "Path(\"input.txt\").write_text(combined, encoding=\"utf-8\")\n",
        "print(\"Wrote input.txt with\", len(combined), \"characters and\", len(BOOKS), \"books.\")"
      ],
      "metadata": {
        "id": "mBbp84jz0rFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61e28bc-35dd-46a5-cd88-1c5591e6c70f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote input.txt with 1593384 characters and 3 books.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Transformer model"
      ],
      "metadata": {
        "id": "_vf_21cdc6FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "wswb7cNkcdEr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MHxXkIjdLGb",
        "outputId": "85ab0cb2-5b2c-4521-87a2-7e041f5a74cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "ZEDFPYrfdNTh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))\n",
        "vocab_size = len(characters)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "char_to_idx = { ch:i for i,ch in enumerate(characters) }\n",
        "idx_to_char = { i:ch for i,ch in enumerate(characters) }\n",
        "encode = lambda xs: [char_to_idx[x] for x in xs]\n",
        "decode = lambda xs: ''.join([idx_to_char[x] for x in xs])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMT7weigdP3w",
        "outputId": "adb8b614-ce6c-45f4-b58d-5fc246de4907"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(len(text) * 0.9)\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]"
      ],
      "metadata": {
        "id": "0HVyoDPMdSmq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split, batch_size, context_size):\n",
        "    data_src = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_src) - context_size, (batch_size,))\n",
        "    x = torch.stack([data_src[i:i+context_size] for i in ix])\n",
        "    y = torch.stack([data_src[i+1:i+context_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "bosufYzbdUvx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, context_size, start_idx, number_of_tokens, temperature=0.9, top_p=0.9):\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "    idx = start_idx\n",
        "    model.eval()\n",
        "    for _ in range(number_of_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / max(1e-6, temperature)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # nucleus sampling\n",
        "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
        "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
        "        mask = cum > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_probs[mask] = 0\n",
        "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "        next_local = torch.multinomial(sorted_probs, 1)\n",
        "        next_token = sorted_idx.gather(-1, next_local)\n",
        "\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "mFG3elkFdXEm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, batch_size, context_size, eval_iters=100):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size, context_size)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "v5S0jFF8dYwW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, steps, batch_size, context_size, report_frequency=1000, lr=1e-3):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    for step in range(steps):\n",
        "        xb, yb = get_batch('train', batch_size, context_size)\n",
        "        _, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step % report_frequency == 0 or step == steps - 1:\n",
        "            losses = estimate_loss(model, batch_size, context_size)\n",
        "            print(f\"Step {step}, train loss: {losses['train']:.4f} | val loss: {losses['val']:.4f}\")\n"
      ],
      "metadata": {
        "id": "DHalYT5Tdaf2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generate_print(model, steps=5000, batch_size=32, context_size=8, lr=1e-3):\n",
        "    train(model, steps, batch_size, context_size, lr=lr)\n",
        "    start_idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    max_tokens = 300\n",
        "    out = generate(model, context_size, start_idx=start_idx, number_of_tokens=max_tokens)[0].tolist()\n",
        "    print(\"\\n=== TRANSFORMER SAMPLE OUTPUT ===\\n\")\n",
        "    print(decode(out))"
      ],
      "metadata": {
        "id": "sgochxaKddDL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, context_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,hd)\n",
        "        q = self.query(x) # (B,T,hd)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5   # (B,T,T)   # mentor-style scaling\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)                         # (B,T,hd)\n",
        "        out = wei @ v                             # (B,T,hd)\n",
        "        return out"
      ],
      "metadata": {
        "id": "a5Ixxf_YdlGY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embd, context_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, context_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "C6V-_0n_dn4O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "yr6FwaGTdpkp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, context_size):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size, n_embd, context_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "DBY_y1e5drIT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=32, context_size=8, n_head=4, n_layer=4):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embd, n_head=n_head, context_size=context_size) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f    = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        emb     = self.token_embedding_table(idx)                                   # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
        "        x = emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                                                    # (B,T,V)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "K_p7qtxWdsfz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size   = 32\n",
        "context_size = 256\n",
        "lr           = 3e-4   # note: train_generate_print default is 1e-3 unless passed explicitly\n",
        "n_embd       = 384\n",
        "n_heads      = 6\n",
        "n_layer      = 6\n",
        "\n",
        "# 5) Init, train, and sample\n",
        "m = GPTLanguageModel(\n",
        "    vocab_size,\n",
        "    n_embd=n_embd,\n",
        "    context_size=context_size,\n",
        "    n_head=n_heads,\n",
        "    n_layer=n_layer\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "VF955HRnduub"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generate_print(m, steps=5000, batch_size=batch_size, context_size=context_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdfuYyBMdxg_",
        "outputId": "b53accdc-2283-424b-f226-30f7e1734460"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, train loss: 3.8937 | val loss: 3.8212\n",
            "Step 1000, train loss: 1.4751 | val loss: 1.4510\n",
            "Step 2000, train loss: 1.2539 | val loss: 1.2798\n",
            "Step 3000, train loss: 1.1471 | val loss: 1.2152\n",
            "Step 4000, train loss: 1.0873 | val loss: 1.1898\n",
            "Step 4999, train loss: 1.0397 | val loss: 1.1859\n",
            "\n",
            "=== TRANSFORMER SAMPLE OUTPUT ===\n",
            "\n",
            "\n",
            "the great number of a boat flew and the ship in the morning of the side\n",
            "that the next morning spread and his torch, which was out of the great\n",
            "company we found the ship and the work of the night we found him where he\n",
            "had all lost this fellow shot, he might be found ourselves, and the lions\n",
            "of the de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in m.parameters())\n",
        "print(f\"Total parameters: {total_params/1e6:.2f}M ({total_params:,})\")"
      ],
      "metadata": {
        "id": "QCQW_q7td2Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84152930-7920-4d6d-9182-6345e193a70c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 10.80M (10,802,002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.eval()\n",
        "with torch.no_grad():\n",
        "    start_idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    long_ids = generate(\n",
        "        m, context_size, start_idx=start_idx, number_of_tokens=2000,\n",
        "        temperature=0.95, top_p=0.95\n",
        "    )[0].tolist()"
      ],
      "metadata": {
        "id": "cJ1G6Dc4X8fx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"PIRATE DIALOGUE (use slang: avast, aye, yarrr, matey, scallywag, grog, booty, keelhaul, cutlass, brig, plunder, buccaneer)\\n\"\n",
        "    \"CAPTAIN: Avast ye, lads—pass the grog and mind yer cutlasses!\\n\"\n",
        "    \"CREW: Aye, cap'n—booty in sight off th' port bow! Yarrr!\\n\"\n",
        "    \"CAPTAIN: \"\n",
        ")"
      ],
      "metadata": {
        "id": "oiz3HXNqAPzh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = [c for c in prompt if c not in char_to_idx]\n",
        "if missing:\n",
        "    print(\"Warning: characters not in vocab replaced with space:\", set(missing))\n",
        "    prompt = \"\".join(c if c in char_to_idx else \" \" for c in prompt)\n",
        "\n",
        "prompt_tokens = torch.tensor([[char_to_idx[c] for c in prompt]], dtype=torch.long, device=device)"
      ],
      "metadata": {
        "id": "Du7LEx6vNBQ6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvXr7k5mNFVB",
        "outputId": "1164bf3c-ccd1-4aa2-ea4c-19e49fd7e117"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIRATE DIALOGUE (use slang: avast, aye, yarrr, matey, scallywag, grog, booty, keelhaul, cutlass, brig, plunder, buccaneer)\n",
            "CAPTAIN: Avast ye, lads—pass the grog and mind yer cutlasses!\n",
            "CREW: Aye, cap'n—booty in sight off th' port bow! Yarrr!\n",
            "CAPTAIN: what if I do not think I was the powers and risked with me. It\n",
            "was the prisoners at last ship of the thicket of the most following more\n",
            "relief. The buccaneers should be so all as if I could not take the best\n",
            "country of the rest. The rest of the whole place in the boat sand in the\n",
            "same strange of the woods and the river Niger ships which was no company\n",
            "of the country and so much as the only sound o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sZlmbEANHHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}